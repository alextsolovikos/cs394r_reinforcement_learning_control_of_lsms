{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45052476",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray.rllib.agents.callbacks import DefaultCallbacks\n",
    "from ray.rllib.env import BaseEnv\n",
    "from ray.rllib.evaluation import Episode, RolloutWorker\n",
    "from ray.rllib.policy import Policy\n",
    "from ray.rllib.policy.sample_batch import SampleBatch\n",
    "\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import ray\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torchsummary import summary\n",
    "\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "\n",
    "from src.envs.multiple_particles_in_flow_continuous import MultipleParticlesInFlowContinuous\n",
    "from src.envs.multiple_particles_in_flow_discrete import MultipleParticlesInFlowDiscrete\n",
    "from src.envs.multiple_particles_in_flow_delayed_obs_continuous import MultipleParticlesInFlowDelayedObsContinuous\n",
    "from src.envs.multiple_particles_in_flow_delayed_obs_discrete import MultipleParticlesInFlowDelayedObsDiscrete\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95578348",
   "metadata": {},
   "source": [
    "# Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e17b3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_and_plot(env, policy, lstm=False):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    step = 0\n",
    "\n",
    "    obs_hist = [obs]\n",
    "    particle_hist = [env.cur_particles]\n",
    "    state_hist = [env.cur_state]\n",
    "    action_hist = []\n",
    "    if lstm:\n",
    "        lstm_state = [np.zeros([lstm_cell_size], np.float32) for _ in range(2)]\n",
    "        lstm_state_hist = [lstm_state]\n",
    "    action = [0]\n",
    "\n",
    "    while not done:\n",
    "        if lstm:\n",
    "            action, lstm_state, _ = policy(obs, lstm_state, explore=False)\n",
    "        else:\n",
    "            action = policy(obs, explore=False)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        step += 1\n",
    "        obs_hist.append(obs)\n",
    "        action_hist.append(action)\n",
    "        particle_hist.append(env.cur_particles)\n",
    "        state_hist.append(env.cur_state)\n",
    "        if lstm:\n",
    "            lstm_state_hist.append(lstm_state)\n",
    "\n",
    "    print(f\"Played 1 episode; total-reward={total_reward}; Number of steps: {step}\")\n",
    "\n",
    "    particle_hist = np.array(particle_hist)\n",
    "    state_hist = np.array(state_hist)\n",
    "    action_hist = np.array(action_hist)\n",
    "    obs_hist = np.array(obs_hist)\n",
    "\n",
    "    act_x, act_y = np.meshgrid(np.linspace(0,15,151),np.linspace(0,1.2,15))\n",
    "    act_mag = env.jet_dist(act_x, act_y)\n",
    "\n",
    "    fig = plt.figure(figsize=(16,8))\n",
    "\n",
    "    ax = fig.add_subplot(211)\n",
    "    ax.plot(particle_hist[:,:,0], particle_hist[:,:,1], c='r', lw=0.1)\n",
    "    ax.scatter(particle_hist[0,:,0], particle_hist[0,:,1], s=5, c='r')\n",
    "    ax.scatter(particle_hist[-1,:,0], particle_hist[-1,:,1], s=5, c='r')\n",
    "    ax.plot(state_hist[:,0], state_hist[:,1], c='darkred', lw=2)\n",
    "    # ax.plot(obs_hist[:,0], obs_hist[:,1], c='r', label='Observation')\n",
    "    # ax.plot(action_region_x, action_region_y, c='orange', label='Force Field')\n",
    "    ax.contourf(act_x, act_y, act_mag, vmin=-1.6, vmax=1.5, cmap='RdBu', zorder=-10)\n",
    "    ax.plot(state_hist[:-1,0], action_hist, c='k', label='Action')\n",
    "    ax.grid(True)\n",
    "    ax.set_aspect('equal', 'box')\n",
    "    ax.set_xlim([0,15])\n",
    "    ax.set_ylim([0,1.2])\n",
    "\n",
    "    ax = fig.add_subplot(212)\n",
    "    ax.plot(action_hist, c='k', label='Action')\n",
    "    ax.plot(state_hist[:-1,1], c='darkred', label='Mean y')\n",
    "    ax.plot(obs_hist, c='b', label='Action')\n",
    "    ax.set_xlim([0,200])\n",
    "    ax.set_ylim([0,1.2])\n",
    "    ax.grid()\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdcca32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(train_episodes, train_reward, train_downwash, train_actuation):\n",
    "    fig = plt.figure(figsize=[16,4])\n",
    "    ax = fig.add_subplot(131)\n",
    "    ax.plot(train_episodes, train_reward, c='r', label='Average Reward')\n",
    "    ax.legend()\n",
    "    ax.grid()\n",
    "\n",
    "    ax = fig.add_subplot(132)\n",
    "    ax.plot(train_episodes, train_downwash, c='b', label='Average Downwash')\n",
    "    ax.legend()\n",
    "    ax.grid()\n",
    "\n",
    "    ax = fig.add_subplot(133)\n",
    "    ax.plot(train_episodes, train_actuation, c='k', label='Average Actuation')\n",
    "    ax.legend()\n",
    "    ax.grid()\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c911044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def const_policy_cont(a, explore=False):\n",
    "    return [1]\n",
    "def const_policy_disc(a, explore=False):\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04eb2bdb",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aaea16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCallbacks(DefaultCallbacks):\n",
    "    def on_episode_start(\n",
    "        self,\n",
    "        *,\n",
    "        worker: RolloutWorker,\n",
    "        base_env: BaseEnv,\n",
    "        policies: Dict[str, Policy],\n",
    "        episode: Episode,\n",
    "        env_index: int,\n",
    "        **kwargs\n",
    "    ):\n",
    "        # Make sure this episode has just been started (only initial obs\n",
    "        # logged so far).\n",
    "        assert episode.length == 0, (\n",
    "            \"ERROR: `on_episode_start()` callback should be called right \"\n",
    "            \"after env reset!\"\n",
    "        )\n",
    "        episode.user_data[\"reward_downwash\"] = []\n",
    "        episode.hist_data[\"reward_downwash\"] = []\n",
    "        episode.user_data[\"reward_actuation\"] = []\n",
    "        episode.hist_data[\"reward_actuation\"] = []\n",
    "        \n",
    "    def on_episode_step(\n",
    "        self,\n",
    "        *,\n",
    "        worker: RolloutWorker,\n",
    "        base_env: BaseEnv,\n",
    "        policies: Dict[str, Policy],\n",
    "        episode: Episode,\n",
    "        env_index: int,\n",
    "        **kwargs\n",
    "    ):\n",
    "        # Make sure this episode is ongoing.\n",
    "        assert episode.length > 0, (\n",
    "            \"ERROR: `on_episode_step()` callback should not be called right \"\n",
    "            \"after env reset!\"\n",
    "        )\n",
    "        reward_downwash = base_env.get_sub_environments()[0].cur_reward_downwash\n",
    "        reward_actuation = -base_env.get_sub_environments()[0].cur_reward_actuation\n",
    "\n",
    "        episode.user_data[\"reward_downwash\"].append(reward_downwash)\n",
    "        episode.user_data[\"reward_actuation\"].append(reward_actuation)\n",
    "\n",
    "    def on_episode_end(\n",
    "        self,\n",
    "        *,\n",
    "        worker: RolloutWorker,\n",
    "        base_env: BaseEnv,\n",
    "        policies: Dict[str, Policy],\n",
    "        episode: Episode,\n",
    "        env_index: int,\n",
    "        **kwargs\n",
    "    ):\n",
    "        # Check if there are multiple episodes in a batch, i.e.\n",
    "        # \"batch_mode\": \"truncate_episodes\".\n",
    "        if worker.policy_config[\"batch_mode\"] == \"truncate_episodes\":\n",
    "            # Make sure this episode is really done.\n",
    "            assert episode.batch_builder.policy_collectors[\"default_policy\"].batches[\n",
    "                -1\n",
    "            ][\"dones\"][-1], (\n",
    "                \"ERROR: `on_episode_end()` should only be called \"\n",
    "                \"after episode is done!\"\n",
    "            )\n",
    "        reward_downwash = np.mean(episode.user_data[\"reward_downwash\"])\n",
    "        reward_actuation = np.mean(episode.user_data[\"reward_actuation\"])\n",
    "\n",
    "        episode.custom_metrics[\"reward_downwash\"] = reward_downwash\n",
    "        episode.custom_metrics[\"reward_actuation\"] = reward_actuation\n",
    "        episode.hist_data[\"reward_downwash\"] = episode.user_data[\"reward_downwash\"]\n",
    "        episode.hist_data[\"reward_actuation\"] = episode.user_data[\"reward_actuation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b9d7db",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------\n",
    "# Shoot down MULTIPLE particle - Partially Observed - Maximize Downwash\n",
    "---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4def5c36",
   "metadata": {},
   "source": [
    "## Global Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99bba7e",
   "metadata": {},
   "source": [
    "## PPO + LSTM - Discrete - W = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40314877",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_name = \"act_cost_5_n_cells_1_1\"\n",
    "\n",
    "global_env_config = {\n",
    "    \"dt\": 0.1,\n",
    "    \"xc\": 9.0,\n",
    "    \"yc\": 0.0,\n",
    "    \"sigma_x\": 0.6,\n",
    "    \"sigma_y\": 0.5,\n",
    "    \"gain\": 0.25,\n",
    "    \"act_cost_weight\": 5.0,\n",
    "    \"obs_min\": [7.0, 0.1],\n",
    "    \"obs_max\": [8.0, 0.5],\n",
    "    \"n_particles\": 400,\n",
    "    \"lamb\": 0.1,\n",
    "    \"n_cells\": [1,1]\n",
    "}\n",
    "\n",
    "env_config = global_env_config.copy()\n",
    "\n",
    "for random_seed in range(100,105):\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    lstm_cell_size = 16\n",
    "    trainer_ppo_lstm_disc = PPOTrainer(\n",
    "        config={\n",
    "            \"env\": MultipleParticlesInFlowDiscrete,\n",
    "            \"env_config\": env_config,\n",
    "            \"framework\": \"torch\",\n",
    "            \"train_batch_size\": 2*1024,\n",
    "            \"callbacks\": MyCallbacks,\n",
    "            # Parallel rollouts\n",
    "    #         \"num_workers\": 8,\n",
    "            \"num_gpus\": 1,\n",
    "            \"lr\": 0.00005,\n",
    "            \"clip_param\": 0.99,\n",
    "            \"log_level\": \"INFO\",\n",
    "            \"model\": {\n",
    "                \"use_lstm\": True,\n",
    "                \"lstm_cell_size\": lstm_cell_size,\n",
    "                \"lstm_use_prev_action\": False,\n",
    "                \"lstm_use_prev_reward\": False,\n",
    "                \"max_seq_len\": 40,\n",
    "                \"fcnet_hiddens\": [256, 256],\n",
    "            },\n",
    "        }\n",
    "    )\n",
    "\n",
    "    train_reward_hist_lstm_disc = []\n",
    "    train_downwash_hist_lstm_disc = []\n",
    "    train_actuation_hist_lstm_disc = []\n",
    "    train_episodes_total_lstm_disc = []\n",
    "\n",
    "    for i in range(400):\n",
    "        results = trainer_ppo_lstm_disc.train()\n",
    "        train_reward_hist_lstm_disc.append(results['episode_reward_mean'])\n",
    "        train_downwash_hist_lstm_disc.append(results['custom_metrics']['reward_downwash_mean'])\n",
    "        train_actuation_hist_lstm_disc.append(results['custom_metrics']['reward_actuation_mean'])\n",
    "        train_episodes_total_lstm_disc.append(results['episodes_total'])\n",
    "        print(f\"i={i}; reward={results['episode_reward_mean']}; downwash={results['custom_metrics']['reward_downwash_mean']}; actuation={results['custom_metrics']['reward_actuation_mean']}; episodes={results['episodes_this_iter']}; total episodes={results['episodes_total']}\")\n",
    "        if i % 10 == 9:\n",
    "            trainer_ppo_lstm_disc.save_checkpoint(trainer_ppo_lstm_disc.logdir)\n",
    "\n",
    "    # Save results to file\n",
    "    train_lstm_disc_results = {\n",
    "        \"train_reward\": train_reward_hist_lstm_disc,\n",
    "        \"train_downwash\": train_downwash_hist_lstm_disc,\n",
    "        \"train_actuation\": train_actuation_hist_lstm_disc,\n",
    "        \"train_episodes\": train_episodes_total_lstm_disc\n",
    "    }\n",
    "\n",
    "    filename = f\"ray_results/ppo_lstm_discrete_{test_name}/train_results_{random_seed}.pkl\"\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(train_lstm_disc_results, f)\n",
    "    trainer_ppo_lstm_disc.save_checkpoint(f\"ray_results/ppo_lstm_discrete_{test_name}\")\n",
    "    \n",
    "    plot_results(train_episodes_total_lstm_disc, train_reward_hist_lstm_disc, train_downwash_hist_lstm_disc, train_actuation_hist_lstm_disc)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2a98d0",
   "metadata": {},
   "source": [
    "## PPO + LSTM - Discrete - W = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d939944",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_name = \"act_cost_10_n_cells_1_1\"\n",
    "\n",
    "global_env_config = {\n",
    "    \"dt\": 0.1,\n",
    "    \"xc\": 9.0,\n",
    "    \"yc\": 0.0,\n",
    "    \"sigma_x\": 0.6,\n",
    "    \"sigma_y\": 0.5,\n",
    "    \"gain\": 0.25,\n",
    "    \"act_cost_weight\": 10.0,\n",
    "    \"obs_min\": [7.0, 0.1],\n",
    "    \"obs_max\": [8.0, 0.5],\n",
    "    \"n_particles\": 400,\n",
    "    \"lamb\": 0.1,\n",
    "    \"n_cells\": [1,1]\n",
    "}\n",
    "\n",
    "env_config = global_env_config.copy()\n",
    "\n",
    "for random_seed in range(100,105):\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    lstm_cell_size = 16\n",
    "    trainer_ppo_lstm_disc = PPOTrainer(\n",
    "        config={\n",
    "            \"env\": MultipleParticlesInFlowDiscrete,\n",
    "            \"env_config\": env_config,\n",
    "            \"framework\": \"torch\",\n",
    "            \"train_batch_size\": 2*1024,\n",
    "            \"callbacks\": MyCallbacks,\n",
    "            # Parallel rollouts\n",
    "    #         \"num_workers\": 8,\n",
    "            \"num_gpus\": 1,\n",
    "            \"lr\": 0.00005,\n",
    "            \"clip_param\": 0.99,\n",
    "            \"log_level\": \"INFO\",\n",
    "            \"model\": {\n",
    "                \"use_lstm\": True,\n",
    "                \"lstm_cell_size\": lstm_cell_size,\n",
    "                \"lstm_use_prev_action\": False,\n",
    "                \"lstm_use_prev_reward\": False,\n",
    "                \"max_seq_len\": 40,\n",
    "                \"fcnet_hiddens\": [256, 256],\n",
    "            },\n",
    "        }\n",
    "    )\n",
    "\n",
    "    train_reward_hist_lstm_disc = []\n",
    "    train_downwash_hist_lstm_disc = []\n",
    "    train_actuation_hist_lstm_disc = []\n",
    "    train_episodes_total_lstm_disc = []\n",
    "\n",
    "    for i in range(400):\n",
    "        results = trainer_ppo_lstm_disc.train()\n",
    "        train_reward_hist_lstm_disc.append(results['episode_reward_mean'])\n",
    "        train_downwash_hist_lstm_disc.append(results['custom_metrics']['reward_downwash_mean'])\n",
    "        train_actuation_hist_lstm_disc.append(results['custom_metrics']['reward_actuation_mean'])\n",
    "        train_episodes_total_lstm_disc.append(results['episodes_total'])\n",
    "        print(f\"i={i}; reward={results['episode_reward_mean']}; downwash={results['custom_metrics']['reward_downwash_mean']}; actuation={results['custom_metrics']['reward_actuation_mean']}; episodes={results['episodes_this_iter']}; total episodes={results['episodes_total']}\")\n",
    "        if i % 10 == 9:\n",
    "            trainer_ppo_lstm_disc.save_checkpoint(trainer_ppo_lstm_disc.logdir)\n",
    "\n",
    "    # Save results to file\n",
    "    train_lstm_disc_results = {\n",
    "        \"train_reward\": train_reward_hist_lstm_disc,\n",
    "        \"train_downwash\": train_downwash_hist_lstm_disc,\n",
    "        \"train_actuation\": train_actuation_hist_lstm_disc,\n",
    "        \"train_episodes\": train_episodes_total_lstm_disc\n",
    "    }\n",
    "\n",
    "    filename = f\"ray_results/ppo_lstm_discrete_{test_name}/train_results_{random_seed}.pkl\"\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(train_lstm_disc_results, f)\n",
    "    trainer_ppo_lstm_disc.save_checkpoint(f\"ray_results/ppo_lstm_discrete_{test_name}\")\n",
    "    \n",
    "    plot_results(train_episodes_total_lstm_disc, train_reward_hist_lstm_disc, train_downwash_hist_lstm_disc, train_actuation_hist_lstm_disc)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfea88c",
   "metadata": {},
   "source": [
    "## PPO + LSTM - Discrete - W = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ee172e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_name = \"act_cost_20_n_cells_1_1\"\n",
    "\n",
    "global_env_config = {\n",
    "    \"dt\": 0.1,\n",
    "    \"xc\": 9.0,\n",
    "    \"yc\": 0.0,\n",
    "    \"sigma_x\": 0.6,\n",
    "    \"sigma_y\": 0.5,\n",
    "    \"gain\": 0.25,\n",
    "    \"act_cost_weight\": 20.0,\n",
    "    \"obs_min\": [7.0, 0.1],\n",
    "    \"obs_max\": [8.0, 0.5],\n",
    "    \"n_particles\": 400,\n",
    "    \"lamb\": 0.1,\n",
    "    \"n_cells\": [1,1]\n",
    "}\n",
    "\n",
    "env_config = global_env_config.copy()\n",
    "\n",
    "for random_seed in range(100,105):\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    lstm_cell_size = 16\n",
    "    trainer_ppo_lstm_disc = PPOTrainer(\n",
    "        config={\n",
    "            \"env\": MultipleParticlesInFlowDiscrete,\n",
    "            \"env_config\": env_config,\n",
    "            \"framework\": \"torch\",\n",
    "            \"train_batch_size\": 2*1024,\n",
    "            \"callbacks\": MyCallbacks,\n",
    "            # Parallel rollouts\n",
    "    #         \"num_workers\": 8,\n",
    "            \"num_gpus\": 1,\n",
    "            \"lr\": 0.00005,\n",
    "            \"clip_param\": 0.99,\n",
    "            \"log_level\": \"INFO\",\n",
    "            \"model\": {\n",
    "                \"use_lstm\": True,\n",
    "                \"lstm_cell_size\": lstm_cell_size,\n",
    "                \"lstm_use_prev_action\": False,\n",
    "                \"lstm_use_prev_reward\": False,\n",
    "                \"max_seq_len\": 40,\n",
    "                \"fcnet_hiddens\": [256, 256],\n",
    "            },\n",
    "        }\n",
    "    )\n",
    "\n",
    "    train_reward_hist_lstm_disc = []\n",
    "    train_downwash_hist_lstm_disc = []\n",
    "    train_actuation_hist_lstm_disc = []\n",
    "    train_episodes_total_lstm_disc = []\n",
    "\n",
    "    for i in range(400):\n",
    "        results = trainer_ppo_lstm_disc.train()\n",
    "        train_reward_hist_lstm_disc.append(results['episode_reward_mean'])\n",
    "        train_downwash_hist_lstm_disc.append(results['custom_metrics']['reward_downwash_mean'])\n",
    "        train_actuation_hist_lstm_disc.append(results['custom_metrics']['reward_actuation_mean'])\n",
    "        train_episodes_total_lstm_disc.append(results['episodes_total'])\n",
    "        print(f\"i={i}; reward={results['episode_reward_mean']}; downwash={results['custom_metrics']['reward_downwash_mean']}; actuation={results['custom_metrics']['reward_actuation_mean']}; episodes={results['episodes_this_iter']}; total episodes={results['episodes_total']}\")\n",
    "        if i % 10 == 9:\n",
    "            trainer_ppo_lstm_disc.save_checkpoint(trainer_ppo_lstm_disc.logdir)\n",
    "\n",
    "    # Save results to file\n",
    "    train_lstm_disc_results = {\n",
    "        \"train_reward\": train_reward_hist_lstm_disc,\n",
    "        \"train_downwash\": train_downwash_hist_lstm_disc,\n",
    "        \"train_actuation\": train_actuation_hist_lstm_disc,\n",
    "        \"train_episodes\": train_episodes_total_lstm_disc\n",
    "    }\n",
    "\n",
    "    filename = f\"ray_results/ppo_lstm_discrete_{test_name}/train_results_{random_seed}.pkl\"\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(train_lstm_disc_results, f)\n",
    "    trainer_ppo_lstm_disc.save_checkpoint(f\"ray_results/ppo_lstm_discrete_{test_name}\")\n",
    "    \n",
    "    plot_results(train_episodes_total_lstm_disc, train_reward_hist_lstm_disc, train_downwash_hist_lstm_disc, train_actuation_hist_lstm_disc)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5694706f",
   "metadata": {},
   "source": [
    "## PPO + LSTM - Discrete - W = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144d1a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_name = \"act_cost_1_n_cells_1_1\"\n",
    "\n",
    "global_env_config = {\n",
    "    \"dt\": 0.1,\n",
    "    \"xc\": 9.0,\n",
    "    \"yc\": 0.0,\n",
    "    \"sigma_x\": 0.6,\n",
    "    \"sigma_y\": 0.5,\n",
    "    \"gain\": 0.25,\n",
    "    \"act_cost_weight\": 1.0,\n",
    "    \"obs_min\": [7.0, 0.1],\n",
    "    \"obs_max\": [8.0, 0.5],\n",
    "    \"n_particles\": 400,\n",
    "    \"lamb\": 0.1,\n",
    "    \"n_cells\": [1,1]\n",
    "}\n",
    "\n",
    "env_config = global_env_config.copy()\n",
    "\n",
    "for random_seed in range(100,105):\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    lstm_cell_size = 16\n",
    "    trainer_ppo_lstm_disc = PPOTrainer(\n",
    "        config={\n",
    "            \"env\": MultipleParticlesInFlowDiscrete,\n",
    "            \"env_config\": env_config,\n",
    "            \"framework\": \"torch\",\n",
    "            \"train_batch_size\": 2*1024,\n",
    "            \"callbacks\": MyCallbacks,\n",
    "            # Parallel rollouts\n",
    "    #         \"num_workers\": 8,\n",
    "            \"num_gpus\": 1,\n",
    "            \"lr\": 0.00005,\n",
    "            \"clip_param\": 0.99,\n",
    "            \"log_level\": \"INFO\",\n",
    "            \"model\": {\n",
    "                \"use_lstm\": True,\n",
    "                \"lstm_cell_size\": lstm_cell_size,\n",
    "                \"lstm_use_prev_action\": False,\n",
    "                \"lstm_use_prev_reward\": False,\n",
    "                \"max_seq_len\": 40,\n",
    "                \"fcnet_hiddens\": [256, 256],\n",
    "            },\n",
    "        }\n",
    "    )\n",
    "\n",
    "    train_reward_hist_lstm_disc = []\n",
    "    train_downwash_hist_lstm_disc = []\n",
    "    train_actuation_hist_lstm_disc = []\n",
    "    train_episodes_total_lstm_disc = []\n",
    "\n",
    "    for i in range(400):\n",
    "        results = trainer_ppo_lstm_disc.train()\n",
    "        train_reward_hist_lstm_disc.append(results['episode_reward_mean'])\n",
    "        train_downwash_hist_lstm_disc.append(results['custom_metrics']['reward_downwash_mean'])\n",
    "        train_actuation_hist_lstm_disc.append(results['custom_metrics']['reward_actuation_mean'])\n",
    "        train_episodes_total_lstm_disc.append(results['episodes_total'])\n",
    "        print(f\"i={i}; reward={results['episode_reward_mean']}; downwash={results['custom_metrics']['reward_downwash_mean']}; actuation={results['custom_metrics']['reward_actuation_mean']}; episodes={results['episodes_this_iter']}; total episodes={results['episodes_total']}\")\n",
    "        if i % 10 == 9:\n",
    "            trainer_ppo_lstm_disc.save_checkpoint(trainer_ppo_lstm_disc.logdir)\n",
    "\n",
    "    # Save results to file\n",
    "    train_lstm_disc_results = {\n",
    "        \"train_reward\": train_reward_hist_lstm_disc,\n",
    "        \"train_downwash\": train_downwash_hist_lstm_disc,\n",
    "        \"train_actuation\": train_actuation_hist_lstm_disc,\n",
    "        \"train_episodes\": train_episodes_total_lstm_disc\n",
    "    }\n",
    "\n",
    "    filename = f\"ray_results/ppo_lstm_discrete_{test_name}/train_results_{random_seed}.pkl\"\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(train_lstm_disc_results, f)\n",
    "    trainer_ppo_lstm_disc.save_checkpoint(f\"ray_results/ppo_lstm_discrete_{test_name}\")\n",
    "    \n",
    "    plot_results(train_episodes_total_lstm_disc, train_reward_hist_lstm_disc, train_downwash_hist_lstm_disc, train_actuation_hist_lstm_disc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cf4c18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tbl_env",
   "language": "python",
   "name": "tbl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
